---
layout: post
title: Logging Hashi@Home
date: 2022-07-11 12:00 +0100
headline: Deploying Loki for logging
categories:
  - blog
tags:
  - observability
  - logging
  - promtail
  - loki
  - consul
  - vault
  - terraform
  - nomad
---

In this post, we look at deploying a logging layer on (and for) Hashi@Home.
As one of the commonly-identified triad of an observability platform, it is an important part of any production environment.
In [Monitoring]({% post_url 2022-01-01-monitoring %}) we went through the motions of selecting what we would use for monitoring, and Prometheus came out on top.

After doing a bit more research, I decided that the logging component for Hashi at Home would be based on Grafana Loki, which would essentially play nicely with Prometheus.

## Logging Game Plan

Loki works similarly to Prometheus in that an agent on an endpoint collects data (logs in this case, metrics in the case of Prometheus), which are then centralised to a central storage system.

The overall plan then is to deploy the log scrapers first as a system job, and then the actual logging system, Loki itself, once we are actually ready to collect data.

So, we would have a `promtail.nomad` system job and a `loki.nomad` service job.
Let's take a closer look.

## Promtail

We want the promtail job to run on all of the nodes, with a restart if the job fails.
While the job could be run as a container, that would mean a bunch of extra configuration that I didn't want to deal with initially, so I chose the `raw_exec` driver so that the job would have direct access to system logs as well as container logs.

The job would take only one variable the Promtail version.
As mentioned before, we need to use the [`system`](https://developer.hashicorp.com/nomad/docs/schedulers#system) job type so that Nomad would schedule executions across all available nodes.

Since promtail exposes a server, to which Loki will eventually be a client, we can also define a service which can be registered with [Consul](https://developer.hashicorp.com/nomad/docs/job-specification/service).
These will be the HTTP endpoint, which has its readiness probe -- we'll use that to register a health check.
Finally, the task needs an artifact to actually execute and a configuration file to launch it with.
We will define the Nomad task with these parameters and use a Nomad template to deliver it to the execution environment when the job is scheduled.

Putting it all together, the Promtail Nomad job looks as follows:

{% highlight hcl %}
# promtail.nomad -- job definition for Promtail

variable "promtail_version" {}

job "promtail" {

  type = "system"
  group "promtail" {
    network {
      port "http" {}
      port "grpc" {}
    }

    service {
      name = "promtail"
      port = "http"

      check {
        name     = "promtail-alive"
        type     = "tcp"
      }

      check {
        name = "Promtail HTTP"
        type = "http"
        path = "/ready"
        port = "http"
      }
    }

    task "promtail" {
      driver = "raw_exec"
      config {
        command = "promtail"
        args = ["-config.file=local/promtail.yml"]
      }

      artifact {
        source = "https://github.com/grafana/loki/releases/download/v${var.promtail_version}/promtail-linux-${attr.cpu.arch}.zip"
        destination = "local/promtail"
        mode = "file"
      }
      template {
         data          = file("templates/promtail.yml.tpl")
         destination   = "local/promtail.yml"
         change_mode   = "signal"
         change_signal = "SIGHUP"
      }
    }
  }
}
{% endhighlight %}

### Promtail configuration template

In the `task` stanza above, we have two pieces of input data which are required in order to run promtail:

1. The promtail executable itself
1. The promtail configuration file

The former is obtained by selecting the version we want to deploy, and constructing a URL to retrieve it from; the promtail binaries are conveniently packaged by the maintainer and published via a Github releases URL.
All we need to decide on is the version and the build architecture.
We use the Nomad node attributes to ensure that the job is portable across the whole set of computatoms, which may have ARM or ARM64 CPU architectures.

The [configuration file](https://grafana.com/docs/loki/latest/send-data/promtail/configuration/#global) however needs to be created for our specific deployment.
We need to tell promtail which port to serve on, and the logging level we desire for the service, and which clients[^loki_clients] it should eventually send data to.

For this we use a consul template to allow the job to be dynamically configure.

The server definition takes into account the dynamic port assigned to the job by Nomad itself, by using the [Nomad runtime variables](https://developer.hashicorp.com/nomad/docs/runtime/environment):

{% highlight hcl %}
server:
  log_level: info
  http_listen_port: {%raw%}{{ env "NOMAD_PORT_http" }}{%endraw%}
  grpc_listen_port: {%raw%}{{ env "NOMAD_PORT_grpc" }}{%endraw%}
{% endhighlight %}

For the client definition, we use a different trick, relying on the Fabio load balancer deployed across all nodes.
Taking advantage of the integration between Consul and Fabio, this allows services to be called via the proxy running on localhost[^no_docker], simply by service name:

{% highlight hcl %}
clients:
  - url: http://localhost:9999/loki/loki/api/v1/push
{% endhighlight %}

Most importantly though, **we need to tell promtail which logs to scrape**.
Somewhat ironically, this is the least "templated" part of the configuration, with just a static definition of scrape configs to pass to promtail.
I wanted to start with monitoring the infrastructure layers, which includes Consul and Nomad, so I defined those scrape configuration targets with a hardcoded `__path__` value.
Of course, OS-level logging is also interesting, so I wanted to scrape journal logs.
Putting it all together, we get:

{% highlight yaml %}
scrape_configs:
  - job_name: consul
    static_configs:
    - targets:
        - localhost
      labels:
        job: consul
        __path__: /home/consul/*.log
  - job_name: nomad
    static_configs:
    - targets:
        - localhost
      labels:
        job: nomad
        __path__: /var/log/nomad*.log
  - job_name: journal
    journal:
      json: false
      max_age: 12h
      path: /var/log/journal
      matches: _TRANSPORT=kernel
      labels:
        job: systemd-journal
    relabel_configs:
      - source_labels: ['__journal__systemd_unit']
        target_label: 'unit'
      - source_labels: ['__journal_syslog_identifier']
        target_label: 'syslog_identifier'
      - source_labels:
          - __journal__hostname
        target_label: nodename
{% endhighlight %}

Unfortunately, promtail doesn't support scraping of Nomad job logs just yet, but I suppose I could scrape the allocations directory, according to [this post on the forum](https://discuss.hashicorp.com/t/general-recommendations-for-logging/40012/3)[^sidecar].

### Promtail deployment

Great, now we have a job definition, a somewhat Submitting this to our Nomad cluster at home, we get the following plan:

{% highlight bash %}
+ Job: "promtail"
+ Task Group: "promtail" (11 create)
  + Task: "promtail" (forces create)

Scheduler dry-run:
- All tasks successfully allocated.

Job Modify Index: 0
To submit the job with version verification run:

nomad job run -check-index 0 promtail.nomad

When running the job with the check-index flag, the job will only be run if the
job modify index given matches the server-side version. If the index has
changed, another user has modified the job and the plan's results are
potentially invalid.
{% endhighlight %}

After the deployment, we can see that it's running on all nodes:

{% highlight bash %}
nomad status promtail
ID            = promtail
Name          = promtail
Submit Date   = 2023-10-22T22:46:34+02:00
Type          = system
Priority      = 50
Datacenters   = dc1
Namespace     = default
Node Pool     = default
Status        = running
Periodic      = false
Parameterized = false

Summary
Task Group  Queued  Starting  Running  Failed  Complete  Lost  Unknown
promtail    0       0         12       8       14        3     0

Allocations
ID        Node ID   Task Group  Version  Desired  Status    Created    Modified
ad425924  35249003  promtail    2        run      running   3m8s ago   2m37s ago
7e477038  d198c25e  promtail    2        run      running   3m39s ago  3m26s ago
412271ec  02195421  promtail    2        run      running   4m40s ago  4m22s ago
91afc845  6ce27c1c  promtail    2        run      running   4m45s ago  4m30s ago
e6e6e97c  786edced  promtail    2        run      running   1d3h ago   1d3h ago
4d9cf48d  f92bedb4  promtail    2        run      running   1d22h ago  1d22h ago
6c6d1711  ffbe483d  promtail    2        run      running   1d22h ago  1d22h ago
0885514a  cd25a978  promtail    2        run      running   1d22h ago  1d22h ago
a538b97d  0243e238  promtail    2        run      running   1d22h ago  1d22h ago
13e91861  9f73ab3a  promtail    2        run      running   1d22h ago  1d22h ago
e79f7f66  7e421864  promtail    2        run      running   1d22h ago  1d22h ago
ef84159a  a0ee4694  promtail    2        run      running   1d22h ago  1d22h ago
{% endhighlight %}

Nice. Let's move on to Loki proper.

## Loki

---

## Footnotes and References

[^loki_clients]: One may imagine several log aggregators, serving partitions. We only define one, but where there are multiple datacentres in a hybrid setup, we might want to consider making promtail targets exposing data only to specific Loki instances.
[^no_docker]: This trick is decidedly less elegant when running the jobs in Docker containers, since they don't see the Fabio service running in localhost, another reason to love Nomad and use simple execution models.
[^sidecar]: Nomad's documentation mentions the [log shipper sidecar pattern](https://developer.hashicorp.com/nomad/tutorials/manage-jobs/jobs-accessing-logs#consider-the-log-shipper-pattern). I guess one could collect logs and ship them to loki like that.
