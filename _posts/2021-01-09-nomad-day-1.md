---
layout: post
title: "Nomad day 1"
date: 2021-01-03 10:17 +0100
headline: "Time to get on the bicycle and pedal"
tags:
  - ansible
  - hardware
  - HAT
  - applications
  - nomad
  - raspberry-pi
categories:
  - blog
---

This is the second step in the story of Hashi@Home, where we start putting our newly-created Consul cluster to some actual use.
I'll try to outline where I think I'm going with this, by explaining some of the the things I think I need in my home compute environment.

## Workload Orchestration

### Keep your HAT on

Many of the compute bits in my office have special bits of hardware attached -- HAT's in the raspberry pi lingo, standing for [Hardware Attached on Top](https://www.raspberrypi.org/blog/introducing-raspberry-pi-hats/).

Part of my Grand Scheme <i class="fas fa-trademark"></i> was to have **physical objects** in my office to work with me through the day.
I wanted to spend less time staring at a screen and have a bit more sensory input from the physical world.
Even before the plague hit, I was spending way too much time to be considered healthy sitting down in front of a computer.
I wanted something like an old-school alarm clock that I had to _walk across the room_ to touch in order to interact with -- except, like, a _smart_ alarm clock.

So, I started browsing the Pi stores for fun ways to turn my compute bits into actual robots that could be interacted with physically.
At the time of writing, I have:

- one [Sense HAT](https://www.raspberrypi.org/products/sense-hat/)
- two [Scroll HAT minis](https://shop.pimoroni.com/products/scroll-hat-mini)
- one [paper HAT](https://shop.pimoroni.com/products/inky-phat?variant=12549254217811)

<figure class="text-center">
  <a title="the Raspberry Pi Foundation, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Raspberry_Pi_with_Sense_HAT.jpg">
  <img alt="Raspberry Pi with Sense HAT" src="/assets/img/512px-Raspberry_Pi_with_Sense_HAT.jpg"></a>
  <figcaption>Raspberry Pi with Sense HAT (the Raspberry Pi Foundation, CC BY-SA 4.0). Mine is attached to a different model Pi, but you get the idea. It's got lots of sensors and flashy LEDs. Tr&egrave;s groovy.</figcaption>
</figure>

### Putting the HATs to work

In order to do anything useful with these HATs, one typicall uses a Python library designed specifically for the HAT in question, usually written in python[^HATlibs].
My early attempts to put these HATs to work was pretty old school: I'd log into the compute bits[^machines], get the scripts that invoke the respective libraries and run them ad-hoc.
A step further would be to invoke them via a `screen` or even via a `systemd` unit.
However, this makes it quite difficult to have "multi-user" access to the hardware.
If there's a script running, let's say a digital clock on the scroll hat, then nothing else could use that piece of hardware.
I pondered this for a while, and came to the realisation that I need something to _orchestrate payloads_ based on some higher-lying rationale.
This realisation was perhaps the strongest initial impetus to use Nomad for this reason.

## Deployment

Since I'd already deployed Consul on the "datacentre" <i class="far fa-grin-tongue-wink"></i> Nomad would be deployed as a "service", hopefully along with relevant health checks, DNS entries, etc.

Nomad documentation is no exception to the "Hashidocs are awesome" rule, so after a once-over of the introductory tutorials, I felt ready to follow the ["Production dpeloyment guide"](https://learn.hashicorp.com/tutorials/nomad/production-deployment-guide-vm-with-consul?in=nomad/production).

Similarly to Consul, Nomad is a distributed system, with auto-discovery of agents, a single binary executable and well-documented configuration language.
I wanted a topology with a single Nomad server and a set of nomad agents, to be defined later.

A concern was to keep from overloading the limited resources provided by my _computatoms_, so for example I decided to avoid making the Pi-Zero's agents since wouldn't provide much compute power anyway.
In due course, I would need them to be agents of course, since this is where the scroll HATs were attached.

The Hashidocs outline a clear and concise path to getting the server and agents onto the targets and configuring them, but I wanted to follow the _Hashi@Home_ way of doing bootstrap deployment with Ansible.

### Nomad Ansible role and inventory update

The first thing I did was add a new group to my inventory to declare which _computatoms_ would be designated as Nomads:

{% highlight ini %}
[nomad_agents:children]
pi3s

[nomad_servers]
inky.station

{% endhighlight %}

This designated a single Pi3 as the server and all the Pi3s as agents -- of course, this meant that one of them (`inky`) would be running both as agent _and_ server, something which the docs discourage.
Hey, we live on the edge in _Hashi@Home_!

Next, I wrote the role to deploy Nomad and template the configurations for the application itself, as well as the systemd unit which would control it.
The only part worth mentioning was that the `server.hcl` part had to be templated _only_ if the target was a Nomad server, according to the group it was in:

{% highlight yaml %}
- name: Template server configuration
  become: true
  template:
    src: etc/nomad.d/server.hcl.j2
    dest: /etc/nomad.d/server.hcl
  when: (is_nomad_server | default('false')) | bool
{% endhighlight %}

---

Role written, it was then applied to the DaTaCenTrE...

### It works!?

<figure>
  <img src="/assets/img/nomad-consul-ui.png" />
  <figcaption>
  Snapshot of Nomad service auto-registered in Consul, taken from the Consul UI.
  </figcaption>
</figure>

On the Consul UI, we can now see two new services: `nomad` and `nomad-client` which were registered by Nomad itself, thanks to the assignment of the name of the datacentre.
I must say, this is a _very_ comfortable way to deploy a service.
Not that I was surprised, but the DNS also worked:

{% highlight shell %}
22:41:35 in ~/Ops/personal via ansible-2.10
➜ dig @sense.station -p 8600 nomad.service.consul

; @sense.station -p 8600 nomad.service.consul
; (1 server found)
;; global options: +cmd
;; Got answer:
;; HEADER opcode: QUERY, status: NOERROR, id: 48015
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;nomad.service.consul.    IN  A

;; ANSWER SECTION:
nomad.service.consul.  0  IN  A  192.168.1.14

;; Query time: 3 msec
;; SERVER: 192.168.1.16#8600(192.168.1.16)
;; WHEN: Sat Jan 09 22:41:39 CET 2021
;; MSG SIZE  rcvd: 65
{% endhighlight %}

As you can see, we have one node reporting as a Nomad server serivce[^observers].
The client DNS also reported correctly:

{% highlight shell %}
➜ dig +nocmd @sense.station -p 8600 nomad-client.service.consul
;; Got answer:
;; HEADER opcode: QUERY, status: NOERROR, id: 39470
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;nomad-client.service.consul.  IN  A

;; ANSWER SECTION:
nomad-client.service.consul. 0  IN  A  192.168.1.10
nomad-client.service.consul. 0  IN  A  192.168.1.14

;; Query time: 3 msec
;; SERVER: 192.168.1.16#8600(192.168.1.16)
;; WHEN: Sat Jan 09 22:46:22 CET 2021
;; MSG SIZE  rcvd: 88
{% endhighlight %}


---

## Voil&agrave;

And so, in less than 2 hours of work, from scratch, we have Nomad deployed on our "datacentre".
Poking around the user interface, I can see that the clients are registered and ready to accept jobs:

<figure>
  <img src="/assets/img/nomad-ui-pi3.png" />
  <figcaption>
  Snapshot of the Nomad UI showing resource monitor of one of the newly-registered clients.
  </figcaption>
</figure>

### Next

So: we have a Consul cluster, with a Nomad server and agents waiting to accept work.
There is plenty left to do of course, not least properly securing both of these.

I performed the initial bootstrap with Ansible, mostly because there was no suitable Terraform provider to use against my rasbperry pis.
However, now we have some providers which we can indeed Terraform!

The next steps should be written mostly in Terraform.
As a neat side-effect, I also have a resilient backend for the state of this datcentre, in the new Consul cluster KV store.

So, next we will be Terraforming Consul and Vault's ACLs, as well as the first few applications deployed on Nomad.

One uncertainty still remains though, which hopefull will become clear during the next few iterations, and it's all about **change management**[^Fitsm].

> How do we do continuous integration and delivery of changes?

I can see a use for [Waypoint](https://waypointproject.io) here, but I can't exactly see _how_ just yet.

There's [a topic open though, over at the Hashi forum](https://discuss.hashicorp.com/t/how-does-packer-fit-into-waypoint/19339) -- watch that space!

## References and Footnotes

[^HATlibs]: For example, the [Sense HAT library](https://pythonhosted.org/sense-hat/) and [Scroll pHAT mini](https://github.com/pimoroni/scroll-phat-hd)
[^machines]: I would colloquially call computers "machines" in the past, but these things are really too small for that to feel right, so I'm just going to call them compute bits. Maybe I'll call them _computatoms_ later.
[^observers]: Astute observers will note that I'm using the raddest of shells (Zsh), with Ansible-2.10.
[^Fitsm]: I'm coming for you [FitSM!](https://www.fitsm.eu/)
